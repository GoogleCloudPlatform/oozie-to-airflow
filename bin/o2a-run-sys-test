#!/usr/bin/env bash

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

MY_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
set -euo pipefail
O2A_DIR="$( cd "${MY_DIR}/.." && pwd )"

RUN_TEST_CACHE_DIR="${O2A_DIR}/.o2a-run-sys-test-cache-dir"

mkdir -pv "${RUN_TEST_CACHE_DIR}"

# Update short and long options in the complete script
# This way autocomplete will work automagically with all options
# shellcheck source=o2a-run-sys-test-complete
. "${MY_DIR}/o2a-run-sys-test-complete"


CMDNAME="$(basename -- "$0")"

function save_to_file {
    # shellcheck disable=SC2005
    echo "$(eval echo "\$$1")" > "${RUN_TEST_CACHE_DIR}/.$1"
}

function read_from_file {
    cat "${RUN_TEST_CACHE_DIR}/.$1" 2>/dev/null || true
}

# GCP global Variables
GCP_REGION=$(read_from_file GCP_REGION)
export GCP_REGION=${GCP_REGION:=europe-west3}

# Dataproc global variables
DATAPROC_CLUSTER_NAME=$(read_from_file DATAPROC_CLUSTER_NAME)
export DATAPROC_CLUSTER_NAME=${DATAPROC_CLUSTER_NAME:=oozie-51}

DATAPROC_USER=$(read_from_file DATAPROC_USER)
export DATAPROC_USER=${DATAPROC_USER:=${USER:=example_user}}

# Composer global variables
COMPOSER_NAME=$(read_from_file COMPOSER_NAME)
export COMPOSER_NAME=${COMPOSER_NAME:=o2a-integration}

COMPOSER_LOCATION=$(read_from_file COMPOSER_LOCATION)
export COMPOSER_LOCATION=${COMPOSER_LOCATION:=europe-west1}

COMPOSER_DAG_BUCKET=$(read_from_file COMPOSER_DAG_BUCKET)
export COMPOSER_DAG_BUCKET=${COMPOSER_DAG_BUCKET:=""}

COMPOSER_GKE_CLUSTER_NAME=""

COMPOSER_GKE_WORKER_NAME=""

COMPOSER_WEB_UI_URL=""

# Script global variables
LOCAL_APP_NAME=$(read_from_file LOCAL_APP_NAME)
export LOCAL_APP_NAME=${LOCAL_APP_NAME:=}

PHASE=$(read_from_file PHASE)
export PHASE=${PHASE:=convert}

RENDER_AS_DOT="false"

RUN_DATAPROC_SSH="false"

RUN_COMPOSER_SSH="false"

RUN_OOZIE_WEBUI="false"

SETUP_AUTOCOMPLETE="false"

VERBOSE="false"

function prepare_configuration {
    if [[ ! -f "${LOCAL_CONFIG_TEMPLATE}" ]]; then
        echo
        echo "The file ${LOCAL_CONFIG_TEMPLATE} does not exist."
        echo "Skipping preparing configuration"
        echo
    else
        echo
        echo "Preparing ${LOCAL_CONFIG} "
        echo "     from ${LOCAL_CONFIG_TEMPLATE}."
        echo
        export DATAPROC_CLUSTER_NAME GCP_REGION COMPOSER_DAG_BUCKET
        echo
        echo "Cluster name: ${DATAPROC_CLUSTER_NAME}"
        echo "Region: ${GCP_REGION}"
        echo
        j2 "${LOCAL_CONFIG_TEMPLATE}" \
              -e DATAPROC_CLUSTER_NAME \
              -e GCP_REGION \
              -e COMPOSER_DAG_BUCKET \
              -e DATAPROC_USER \
              -e LOCAL_APP_NAME \
              -o "${LOCAL_CONFIG}"
    fi
}

function prepare_dataproc {
    header "Prepare HDFS ${HDFS_REMOTE_APP_DIR} via master's ${DATAPROC_FILESYSTEM_ALL_APPS_DIR} \
for example application: ${LOCAL_APP_NAME}"

    run_command_on_dataproc "rm -rf ${DATAPROC_FILESYSTEM_APP_DIR}"
    if [[ -d "${LOCAL_APP_DIR}/hdfs" ]]; then
        if [[ ! -f "${LOCAL_APP_DIR}/hdfs/workflow.xml" ]]; then
            echo
            echo "Missing ${LOCAL_APP_DIR}/hdfs/workflow.xml file"
            echo
            exit 1
        fi
        run_command_on_dataproc "mkdir -p \"${DATAPROC_FILESYSTEM_ALL_APPS_DIR}\""
        scp_folder_to_dataproc "${LOCAL_APP_DIR}/hdfs/" "${DATAPROC_FILESYSTEM_APP_DIR}"
    else
        echo
        echo "Missing ${LOCAL_APP_DIR}/hdfs folder. It should contain at least workflow.xml"
        echo
        exit 1
    fi

    LOCAL_APP_PROPERTIES="/tmp/${LOCAL_APP_NAME}_${RANDOM_PREFIX}_job.properties"

    # Overwrite job properties with the dataproc cluster master

    sed -e "s/^nameNode.*$/nameNode=hdfs:\/\/${DATAPROC_CLUSTER_MASTER}:8020/" \
        -e "s/^resourceManager.*$/resourceManager=${DATAPROC_CLUSTER_MASTER}:8032/" \
        -e "s/\${user\.name}/${DATAPROC_USER}/" \
        "${LOCAL_APP_DIR}/job.properties"  > "${LOCAL_APP_PROPERTIES}"
    scp_file_to_dataproc "${LOCAL_APP_PROPERTIES}" "${DATAPROC_FILESYSTEM_APP_PROPERTIES}"

    submit_pig "fs -rm -r -f ${HDFS_REMOTE_APP_DIR}"
    submit_pig "fs -mkdir -p ${HDFS_REMOTE_ALL_APPS_DIR}"

    # Note! The target will be /user/${user.name}/${examplesRoot}/apps/<TEST_APP>
    submit_pig "fs -copyFromLocal ${DATAPROC_FILESYSTEM_APP_DIR} ${HDFS_REMOTE_APP_DIR}"
    footer
}

function prepare_composer {
    header "Prepares Composer by copying reusable library files"
    gsutil -m cp -r "${LOCAL_SCRIPTS_DIR}/*" "gs://${COMPOSER_DAG_BUCKET}/data/"
    gsutil cp -r "${LOCAL_O2A_PROJECT_DIR}/o2a/__init__.py" "gs://${COMPOSER_DAG_BUCKET}/dags/o2a/__init__.py"
    gsutil -m cp -r "${LOCAL_O2A_LIBS_DIR}/*.py" "gs://${COMPOSER_DAG_BUCKET}/dags/o2a/o2a_libs/"
    footer
}

usage() {
      echo """

Usage: ${CMDNAME} [FLAGS] [-A|-S|-K|-W]

Executes prepare or run phase for integration testing of O2A converter.

Flags:

-h, --help
        Shows this help message.

-a, --application <APPLICATION>
        Application (from examples dir) to run the tests on. Must be specified unless -S or -A are specified.
        One of [${_ALLOWED_APPLICATIONS}]

-p, --phase <PHASE>
        Phase of the test to run. One of [${_ALLOWED_PHASES}]. Defaults to ${PHASE}.

-C, --composer-name <COMPOSER_NAME>
        Composer instance used to run the operations on. Defaults to ${COMPOSER_NAME}

-L, --composer-location <COMPOSER_LOCATION>
        Composer locations. Defaults to ${COMPOSER_LOCATION}

-c, --cluster <CLUSTER>
        Cluster used to run the operations on. Defaults to ${DATAPROC_CLUSTER_NAME}

-b, --bucket <BUCKET>
        Airflow Composer DAG bucket used. Defaults to bucket that is used by Composer.

-r, --region <REGION>
        GCP Region where the cluster is located. Defaults to ${GCP_REGION}

-v, --verbose
        Add even more verbosity when running the script.

-d, --dot
        Creates files in the DOT representation.
        If you have the graphviz program in PATH, the files will also be converted to the PNG format.
        If you have the graphviz program and the imgcat programs in PATH, the files will also be displayed in the console

Optional commands to execute:

-K, --ssh-to-composer-worker
        Open shell access to Airflow's worker. This allows you to test commands in the context of the Airflow instance.
        It is worth noting that it is possible to access the database.
        The kubectl exec command is used internally, so not all SSH features are available.

-S, --ssh-to-dataproc-master
        SSH to Dataproc's cluster master. All SSH features are available by this options.
        Arguments after -- are passed to gcloud compute ssh command as extra args.

-W, --open-oozie-web-ui
        Creates a SOCKS5 proxy server that redirects traffic through Dataproc's cluster master and
        opens Google Chrome with a proxy configuration and a tab with the Oozie web interface.

-A, --setup-autocomplete
        Sets up autocomplete for o2a-run-sys-tests

"""
}

echo

set +e

getopt -T >/dev/null
GETOPT_RETVAL=$?

if [[ $(uname -s) == 'Darwin' ]] ; then
    command -v gstat >/dev/null
    STAT_PRESENT=$?
else
    command -v stat >/dev/null
    STAT_PRESENT=$?
fi

command -v j2 > /dev/null
J2_PRESENT=$?

set -e

function setup_autocomplete {
    echo
    echo "Installing bash completion for local user"
    echo
    set +e
    # shellcheck disable=SC2088
    grep "~/.bash_completion.d" ~/.bash_profile >/dev/null 2>&1
    RES=$?
    set -e
    if [[ "${RES}" == "0" ]]; then
        echo "Bash dir already created"
    else
        "${MY_DIR}/o2a-confirm" "This will create ~/.bash_completion.d/ directory and modify ~/.bash_profile file"
        echo
        echo
        mkdir -pv ~/.bash_completion.d
        touch ~/.bash_profile
        cat >>~/.bash_profile <<"EOF"
for BCFILE in ~/.bash_completion.d/* ; do
. ${BCFILE}
done
EOF
    fi
    ln -sf "${MY_DIR}/o2a-run-sys-test-complete" ~/.bash_completion.d/
    echo
    echo
    echo The o2a-run-sys-test completion installed to ~/.bash_completion.d/o2a-run-sys-test-complete
    echo
    echo
    echo "Please re-enter bash or run '. ~/.bash_completion.d/o2a-run-sys-test-complete'"
    echo
}

####################  Parsing options/arguments
if [[ ${GETOPT_RETVAL} != 4 || "${STAT_PRESENT}" != "0" ]]; then
    echo
    if [[ $(uname -s) == 'Darwin' ]] ; then
        echo "You are running ${CMDNAME} in OSX environment"
        echo "And you need to install gnu commands"
        echo
        echo "Run 'brew install gnu-getopt coreutils'"
        echo
        echo "Then link the gnu-getopt to become default as suggested by brew by typing:"
        echo "echo 'export PATH=\"/usr/local/opt/gnu-getopt/bin:\$PATH\"' >> ~/.bash_profile"
        echo ". ~/.bash_profile"
        echo
        echo "Login and logout afterwards"
        echo
    else
        echo "You do not have necessary tools in your path (getopt, stat). Please install the"
        echo "Please install latest/GNU version of getopt and coreutils."
        echo "This can usually be done with 'apt install util-linux coreutils'"
    fi
    echo
    exit 1
fi

####################  Parsing options/arguments
if [[ "${J2_PRESENT}" != "0" ]]; then
    echo
    echo "ERROR! j2 client for jinja2 processing is not present on path"
    echo
    echo
    echo "Please make sure that you are within virtualenv prepared with 'pip install -r requirements.txt'"
    echo
    echo
    exit 1
fi

if ! PARAMS=$(getopt \
    -o "${_SHORT_OPTIONS:=}" \
    -l "${_LONG_OPTIONS:=}" \
    --name "$CMDNAME" -- "$@")
then
    usage
    exit 1
fi


eval set -- "${PARAMS}"
unset PARAMS

# Parse Flags.
# Please update short and long options in the o2a-run-sys-test-complete script
# This way autocomplete will work out-of-the-box
while true
do
  case "${1}" in
    -h|--help)
      usage;
      exit 0 ;;
    -a|--application)
      export LOCAL_APP_NAME="${2}";
      echo
      echo "Application to test: ${LOCAL_APP_NAME}"
      echo
      shift 2 ;;
    -C|--composer-name)
      export COMPOSER_NAME="${2}";
      echo
      echo "Composer name: ${COMPOSER_NAME}"
      echo
      shift 2 ;;
    -L|--composer-location)
      export COMPOSER_LOCATION="${2}";
      echo
      echo "Composer location: ${COMPOSER_LOCATION}"
      echo
      shift 2 ;;
    -p|--phase)
      export PHASE="${2}";
      echo
      echo "Phase to run: ${PHASE}"
      echo
      shift 2 ;;
    -b|--bucket)
      export COMPOSER_DAG_BUCKET="${2}";
      echo
      echo "Composer Bucket where DAGs are stored: ${COMPOSER_DAG_BUCKET}"
      echo
      shift 2 ;;
    -c|--cluster)
      export DATAPROC_CLUSTER_NAME="${2}";
      echo
      echo "Cluster to run test on: ${DATAPROC_CLUSTER_NAME}"
      echo
      shift 2 ;;
    -r|--region)
      export GCP_REGION="${2}";
      echo
      echo "Region where cluster is located: ${GCP_REGION}"
      echo
      shift 2 ;;
    -v|--verbose)
      export VERBOSE="true";
      echo
      echo "Verbosity turned on"
      echo
      shift ;;
    -K|--ssh-to-composer-worker)
      export RUN_COMPOSER_SSH="true"
      echo
      echo "Connecting to Airflow worker"
      echo
      shift ;;
    -S|--ssh-to-dataproc-master)
      export RUN_DATAPROC_SSH="true"
      echo
      echo "Connecting to Dataproc master"
      echo
      shift ;;
    -W|--open-oozie-webui)
      export RUN_OOZIE_WEBUI="true"
      echo
      echo "Opening Oozie Web UI"
      echo
      shift ;;
    -A|--setup-autocomplete)
      export SETUP_AUTOCOMPLETE="true"
      echo
      echo "Setting up autocomplete"
      echo
      shift ;;
    -d|--dot)
      export RENDER_AS_DOT="true"
      echo
      echo "Rendering as DOT files"
      echo
      shift ;;
    --)
      shift ;
      break ;;
    *)
      usage
      echo
      echo "ERROR: Unknown argument ${1}"
      echo
      exit 1
      ;;
  esac
done

function ssh_to_composer_worker {
    echo "Connecting to worker..."
    kubectl exec -it "${COMPOSER_GKE_WORKER_NAME}" --container airflow-worker -- /bin/bash
}

function ssh_to_dataproc_master {
    echo
    echo "SSH to cluster master ${DATAPROC_CLUSTER_MASTER} in zone ${DATAPROC_MASTER_ZONE}"
    echo
    set -x
    # shellcheck disable=SC2068
    gcloud compute ssh "${DATAPROC_CLUSTER_MASTER}" --zone "${DATAPROC_MASTER_ZONE}" -- $@
}

function open_oozie_webui {
    local PROXY_PORT=1080
    local CHROME_PROFILE_TMP="/tmp/o2a-chrome-profile-${DATAPROC_CLUSTER_MASTER}-${DATAPROC_MASTER_ZONE}"
    echo
    echo "Open Oozie Web UI via SOCKS5 proxy on ${PROXY_PORT}."
    echo "You will be connected to cluster master ${DATAPROC_CLUSTER_MASTER} in zone ${DATAPROC_MASTER_ZONE}."
    echo
    local CHROME_BIN
    if [[ -x '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome' ]]; then
        CHROME_BIN='/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
    elif command -v google-chrome >> /dev/null; then
        CHROME_BIN="google-chrome"
    else
        echo "You need to install Google Chrome to use this functionality."
        echo
        echo "For more information look at:"
        echo "https://support.google.com/chrome/answer/95346"
        echo
        exit 1
    fi;
    set -x
    # Start SSH dynamic port forwarding
    gcloud compute ssh  "${DATAPROC_CLUSTER_MASTER}" --zone "${DATAPROC_MASTER_ZONE}" -- -D "${PROXY_PORT}" -N &
    # Open the browser
    "${CHROME_BIN}" --proxy-server="socks5://localhost:${PROXY_PORT}" \
    --user-data-dir="${CHROME_PROFILE_TMP}" --no-default-browser-check \
    "http://${DATAPROC_CLUSTER_MASTER}:11000"  &> /dev/null
    # Delete temporary browser profile
    rm -rf "${CHROME_PROFILE_TMP}"
    # Kill SSH dynamic port forwarding
    kill %1
}

function verify_arguments {
    if [[ ${LOCAL_APP_NAME} == "" ]]; then
        echo
        echo "ERROR! You have not specified an application with --application switch"
        echo
        exit 1
    fi

    #################### Check python version ##########################################
    if [[ ${_ALLOWED_PHASES:=} != *" ${PHASE} "* ]]; then
        echo
        echo "ERROR! Allowed phases are [${_ALLOWED_PHASES}]. Used: '${PHASE}'"
        echo
        usage
        exit 1
    fi

    if [[ ${_ALLOWED_APPLICATIONS:=} != *" ${LOCAL_APP_NAME} "* ]]; then
        echo
        echo "ERROR! Allowed applications are [${_ALLOWED_APPLICATIONS}]. Used: '${LOCAL_APP_NAME}'"
        echo
        usage
        exit 1
    fi
}

function header {
    echo
    SEPARATOR_WIDTH=$(tput cols)
    printf '#%.0s' $(seq "${SEPARATOR_WIDTH}")
    echo
    echo "#    $1"
    printf '#%.0s' $(seq "${SEPARATOR_WIDTH}")
    echo
    echo
    set -x
}

function footer {
    set +x
    SEPARATOR_WIDTH=$(tput cols)
    echo
    printf '#%.0s' $(seq "${SEPARATOR_WIDTH}")
    echo
    echo
}

function run_command_on_composer {
    kubectl exec -t "${COMPOSER_GKE_WORKER_NAME}" --container airflow-worker -- "$@"
}

function run_command_on_dataproc {
    gcloud compute ssh "${DATAPROC_CLUSTER_MASTER}" --zone="${DATAPROC_MASTER_ZONE}" --command "${1}"
}

function scp_folder_to_dataproc {
    gcloud compute scp --zone="${DATAPROC_MASTER_ZONE}" --recurse "${1}" "${DATAPROC_CLUSTER_MASTER}:${2}"
}

function scp_folder_from_dataproc {
    gcloud compute scp --zone="${DATAPROC_MASTER_ZONE}" --recurse "${DATAPROC_CLUSTER_MASTER}:${1}" "${2}"
}

function scp_file_to_dataproc {
    gcloud compute scp --zone="${DATAPROC_MASTER_ZONE}" "${1}" "${DATAPROC_CLUSTER_MASTER}:${2}"
}

function submit_pig {
    gcloud dataproc jobs submit pig --cluster="${DATAPROC_CLUSTER_NAME}" --region="${GCP_REGION}" \
        --execute "${1}"
}

function convert_python {
    header "Convert workflow.xml to Airflow's ${OUTPUT_DAG_NAME}.py dag for example: ${LOCAL_APP_NAME}"
    if [[ ${WITH_COVERAGE} == "true" ]];  then
        coverage run -a \
        "${LOCAL_O2A_BIN_SCRIPT}" \
            --input-directory-path "${LOCAL_APP_DIR}" \
            --output-directory-path "${LOCAL_OUTPUT_DIR}" \
            --user "${DATAPROC_USER}" \
            --dag-name "${OUTPUT_DAG_NAME}"
    else
        "${LOCAL_O2A_BIN_SCRIPT}" \
            --input-directory-path "${LOCAL_APP_DIR}" \
            --output-directory-path "${LOCAL_OUTPUT_DIR}" \
            --user "${DATAPROC_USER}" \
            --dag-name "${OUTPUT_DAG_NAME}"
    fi
    python -m py_compile "${LOCAL_OUTPUT_DIR}/${OUTPUT_DAG_NAME}.py"
    footer
}

function convert_dot {
    header "Convert workflow.xml to dot files for example: ${LOCAL_APP_NAME}"
    "${LOCAL_O2A_BIN_SCRIPT}" \
        --input-directory-path "${LOCAL_APP_DIR}" \
        --output-directory-path "${LOCAL_OUTPUT_DIR}" \
        --user "${DATAPROC_USER}" \
        --dag-name "${OUTPUT_DAG_NAME}" \
        --dot

    set +x
    if command -v dot > /dev/null; then
        echo "Rendering DOT files"
        for dot_file in "${LOCAL_OUTPUT_DIR}"/*.dot; do
            dot -Tpng "${dot_file}" -o "${dot_file%.dot}.png"
        done;
        if  command -v imgcat > /dev/null; then
            echo "Rendering PNG files"
            for png_file in "${LOCAL_OUTPUT_DIR}"/*.png; do
                imgcat -p "${png_file%.dot}"
            done;
        else
            echo "imgcat is not present on path"
            echo "Skip displaying the image"
        fi
    else
        echo "graphviz is not present on path"
        echo "Skip converting to the PNG image"
    fi
    set -x
    footer
}

function convert {
    if [[ ${RENDER_AS_DOT} == "false" ]]; then
        convert_python
    else
        convert_dot
    fi
}

function random_date_in_past {
    CURRENT_TIMESTAMP=$(date +"%s")
    NEW_TIMESTAMP=$((CURRENT_TIMESTAMP-RANDOM-RANDOM))
    date -r ${NEW_TIMESTAMP} '+%Y-%m-%d %H:%M:%S'
}

function test_composer {
    header "Triggers example application on Composer: ${LOCAL_APP_NAME}"
    gsutil cp -r "${LOCAL_OUTPUT_DIR}/*" "gs://${COMPOSER_DAG_BUCKET}/dags/"
    echo "Waiting until the scheduler finds a new dag file."
    sleep 30
    TEST_EXECUTION_DATE="$(random_date_in_past)"
    run_command_on_composer airflow list_dags
    run_command_on_composer airflow trigger_dag "${OUTPUT_DAG_NAME}" -e "${TEST_EXECUTION_DATE}"
    echo
    echo "You can check the progress at the address: ${COMPOSER_WEB_UI_URL}"
    echo
    DAG_STATUS="running"
    while [ "${DAG_STATUS}" == "running" ] ; do
        echo "Waiting for a change in the job status"
        sleep 1
        DAG_STATUS="$(run_command_on_composer airflow dag_state "${OUTPUT_DAG_NAME}" "${TEST_EXECUTION_DATE}" 2> /dev/null \
        | tail -n 1 \
        | sed "s/$(printf '\r')\$//g")"
    done
    if [[ "${DAG_STATUS}" != "success" ]]; then
        echo "The DAG finished with no-success state."
        echo "Current DAG status: ${DAG_STATUS}"
        exit 1
    fi
    echo "The DAG finished with success state."
    footer
}

function test_oozie {
    header "Triggers example application on Oozie: ${LOCAL_APP_NAME}"
    JOB_ID_LINE=$(run_command_on_dataproc "sudo oozie job --config ${DATAPROC_FILESYSTEM_APP_PROPERTIES} -run" \
        | tee /dev/tty \
        | grep '^job: ')
    # Skip prefix
    # "job: " = 5 characters
    JOB_ID="${JOB_ID_LINE:5}"
    set +x
    echo "You can run the following commands cluster to see the job status"
    echo
    echo "${CMDNAME} -S -- oozie job -info ${JOB_ID}"
    echo "${CMDNAME} -S -- oozie job -log ${JOB_ID}"
    echo
    echo "Follow instructions of setting up the proxy in dataproc cluster console to observe the job"
    echo
    echo "Once you setup the proxy, hadoop interface will be available in the browser with proxy at:"
    echo "http://${DATAPROC_CLUSTER_MASTER}:8088"
    echo
    set -x
    JOB_STATUS='RUNNING'
    while [ "${JOB_STATUS}" == "RUNNING" ] ; do
       echo "Waiting for a change in the job status"
       sleep 1
       JOB_STATUS=$(run_command_on_dataproc "oozie job -info ${JOB_ID}" 2> /dev/null \
           | grep -e 'Status\s\+: [A-Z]\+' \
           | cut -d ":" -f 2 | tail -c +2 \
           | sed "s/$(printf '\r')//g")
    done

    if [[ "${JOB_STATUS}" != "SUCCEEDED" ]]; then
        echo "The job finished with no succeeded status."
        echo "Current job status: ${JOB_STATUS}"
        exit 1
    fi
    echo "The job finished with succeeded status."
    footer
}

function download_test_artifacts {
    ENVIRONMENT=${1}

    header "Copying test artifacts for ${ENVIRONMENT^}"

    CURRENT_ARTIFACTS_OUTPUT="${LOCAL_ARTIFACTS_DIR}/${ENVIRONMENT}"
    TMP_LOCAL_PATH="/tmp/o2a-artifacts-$(date | md5sum | head -c 7)"
    submit_pig "fs -copyToLocal ${HDFS_REMOTE_APP_DIR} ${TMP_LOCAL_PATH}"
    rm -rf "${CURRENT_ARTIFACTS_OUTPUT}" || true
    mkdir -p "${CURRENT_ARTIFACTS_OUTPUT}"
    scp_folder_from_dataproc "${TMP_LOCAL_PATH}/*" "${CURRENT_ARTIFACTS_OUTPUT}"
    run_command_on_dataproc "sudo rm -rf ${TMP_LOCAL_PATH}"

    footer
}

function compare_test_artifacts {
    header "Comparing artifacts between Oozie and Airflow for: ${LOCAL_APP_NAME}"

    OOZIE_ARTIFACTS_OUTPUT="${LOCAL_ARTIFACTS_DIR}/oozie"
    COMPOSER_ARTIFACTS_OUTPUT="${LOCAL_ARTIFACTS_DIR}/composer"
    diff -r "${OOZIE_ARTIFACTS_OUTPUT}" "${COMPOSER_ARTIFACTS_OUTPUT}"

    footer
}

function fetch_composer_environment_info {
    header "Fetching information about the Composer environment"
    echo
    if [[ -z "${COMPOSER_DAG_BUCKET}" ]]; then
        COMPOSER_DAG_BUCKET=$(gcloud beta composer environments describe "${COMPOSER_NAME}" --location "${COMPOSER_LOCATION}" --format='value(config.dagGcsPrefix)')
        COMPOSER_DAG_BUCKET=${COMPOSER_DAG_BUCKET%/dags}
        COMPOSER_DAG_BUCKET=${COMPOSER_DAG_BUCKET#gs://}
    fi
    COMPOSER_GKE_CLUSTER_NAME=$(gcloud beta composer environments describe "${COMPOSER_NAME}" --location "${COMPOSER_LOCATION}" '--format=value(config.gkeCluster)' | cut -d '/' -f 6)
    gcloud container clusters get-credentials "${COMPOSER_GKE_CLUSTER_NAME}" --zone europe-west1-b
    COMPOSER_GKE_WORKER_NAME=$(kubectl get pods | grep "airflow-worker" | grep "Running" | head -1 | cut -d " " -f 1)
    if [[ ${COMPOSER_GKE_WORKER_NAME} == "" ]]; then
        echo "No running airflow-worker!"
        exit 1
    fi
    COMPOSER_WEB_UI_URL=$(gcloud beta composer environments describe "${COMPOSER_NAME}" --location "${COMPOSER_LOCATION}" --format='value(config.airflowUri)')
    set +x
    echo
    echo "DAG Bucket:       ${COMPOSER_DAG_BUCKET}"
    echo "GKE Cluster Name: ${COMPOSER_GKE_CLUSTER_NAME}"
    echo "GKE Worker Name:  ${COMPOSER_GKE_WORKER_NAME}"
    echo "WEB UI URL:       ${COMPOSER_WEB_UI_URL}"
    echo
    footer
}

function fetch_dataproc_environment_info {
    header "Fetching information about the Dataproc environment"
    echo
    DATAPROC_CLUSTER_MASTER=$(gcloud dataproc clusters describe "${DATAPROC_CLUSTER_NAME}" --region="${GCP_REGION}" --format='value(config.masterConfig.instanceNames[0])')
    DATAPROC_MASTER_ZONE=$(gcloud dataproc clusters describe "${DATAPROC_CLUSTER_NAME}" --region="${GCP_REGION}" --format='value(config.gceClusterConfig.zoneUri.basename())')
    DATAPROC_USER=$(run_command_on_dataproc "whoami")
    DATAPROC_FILESYSTEM_ALL_APPS_DIR=/home/${DATAPROC_USER}/o2a
    DATAPROC_FILESYSTEM_APP_DIR=${DATAPROC_FILESYSTEM_ALL_APPS_DIR}/${LOCAL_APP_NAME}
    DATAPROC_FILESYSTEM_APP_PROPERTIES=${DATAPROC_FILESYSTEM_ALL_APPS_DIR}/${LOCAL_APP_NAME}.properties
    HDFS_REMOTE_ALL_APPS_DIR=/user/${DATAPROC_USER}/examples/apps
    HDFS_REMOTE_APP_DIR=${HDFS_REMOTE_ALL_APPS_DIR}/${LOCAL_APP_NAME}
    echo
    footer
}

pushd "${O2A_DIR}"

if [[ ${VERBOSE} == "true" ]]; then
    set -x
fi

if [[ ${RUN_DATAPROC_SSH} == "true" ]]; then
    fetch_dataproc_environment_info
    # shellcheck disable=SC2068
    ssh_to_dataproc_master $@
    exit
fi

if [[ ${RUN_COMPOSER_SSH} == "true" ]]; then
    fetch_composer_environment_info
    ssh_to_composer_worker
    exit
fi

if [[ ${RUN_OOZIE_WEBUI} == "true" ]]; then
    fetch_dataproc_environment_info
    open_oozie_webui
    exit
fi

if [[ ${SETUP_AUTOCOMPLETE} == "true" ]]; then
    setup_autocomplete
    exit
fi

WITH_COVERAGE=${WITH_COVERAGE:="false"}
LOCAL_O2A_BIN_SCRIPT="${MY_DIR}/o2a"
LOCAL_O2A_PROJECT_DIR="${O2A_DIR}"
LOCAL_EXAMPLE_DIR=examples

RANDOM_PREFIX=$(echo $RANDOM $RANDOM $RANDOM $RANDOM $RANDOM | md5sum | cut -c -8)

LOCAL_APP_DIR="${LOCAL_O2A_PROJECT_DIR}/${LOCAL_EXAMPLE_DIR}/${LOCAL_APP_NAME}"

LOCAL_OUTPUT_DIR="${O2A_DIR}/output/${LOCAL_APP_NAME}"
LOCAL_ARTIFACTS_DIR="${O2A_DIR}/output-artifacts/${LOCAL_APP_NAME}"
LOCAL_O2A_LIBS_DIR="${LOCAL_O2A_PROJECT_DIR}/o2a/o2a_libs"
LOCAL_SCRIPTS_DIR="${LOCAL_O2A_PROJECT_DIR}/o2a/scripts"
OUTPUT_DAG_NAME="test_${LOCAL_APP_NAME}_dag"

LOCAL_CONFIG_TEMPLATE=${LOCAL_APP_DIR}/configuration.template.properties
LOCAL_CONFIG=${LOCAL_APP_DIR}/configuration.properties

verify_arguments

if _listcontains "${_ALLOWED_PHASES}" "${PHASE}"; then
    echo "Running phase: ${PHASE}"
else
    echo
    echo "Wrong phase specified"
    echo
    exit 1
fi


save_to_file LOCAL_APP_NAME
save_to_file COMPOSER_NAME
save_to_file COMPOSER_LOCATION
save_to_file PHASE
save_to_file COMPOSER_DAG_BUCKET
save_to_file DATAPROC_CLUSTER_NAME
save_to_file GCP_REGION

if [[ "${PHASE}" == "prepare-configuration" ]]; then
    prepare_configuration
    # We only want to prepare configuration
    exit 0
fi

if [[ "${PHASE}" == "convert" ]]; then
    # in case we only convert locally we do not fetch dataproc info (user, cluster master)
    # nor composer environment info and we rely on locally entered data
    prepare_configuration
    convert
elif [[ "${PHASE}" == "prepare-dataproc" ]]; then
    fetch_dataproc_environment_info
    prepare_configuration
    prepare_dataproc
elif [[ "${PHASE}" == "test-composer" ]]; then
    fetch_dataproc_environment_info
    fetch_composer_environment_info
    prepare_configuration
    convert
    prepare_composer
    test_composer
    download_test_artifacts "composer"
elif [[ "${PHASE}" == "test-oozie" ]]; then
    fetch_dataproc_environment_info
    test_oozie
    download_test_artifacts "oozie"
elif [[ "${PHASE}" == "test-compare-artifacts" ]]; then
    fetch_dataproc_environment_info
    fetch_composer_environment_info
    prepare_configuration
    convert
    prepare_composer
    test_composer
    download_test_artifacts "composer"
    test_oozie
    download_test_artifacts "oozie"
    compare_test_artifacts
fi

set +x

popd
